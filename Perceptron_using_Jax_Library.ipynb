{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron using Jax Library.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKLujvszln/lkMQjp5WXS6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebzkurt1/ml_with_jax/blob/main/Perceptron_using_Jax_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_76hMwb1wICX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "from jax import random\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X = jnp.array([[3.0,1.2,3.6],[1.4,6.21,6.8],[1.4,67.3,8.9],[1.4,2.6,1.6]])\n",
        "X = random.normal(key=random.PRNGKey(42),shape=(100,4))\n",
        "# y = jnp.array([[1],[2],[3],[2]])\n",
        "y = jnp.array(\n",
        "    [\n",
        "     [0],[0],[2],[1],[2],[0],[1],[2],[2],[1]\n",
        "    ]\n",
        ")\n",
        "class_num = 3\n",
        "W = random.normal(key=random.PRNGKey(42),shape=(X.shape[-1],class_num))\n",
        "b = random.normal(key=random.PRNGKey(42),shape=(class_num,1))\n",
        "# print(b)\n",
        "# print(jnp.matmul(X,W))\n",
        "# ff = (jnp.matmul(X,W).T + b).T\n",
        "# pred = jnp.exp(ff)/jnp.exp(ff).sum(axis=0)\n",
        "# pred_arg = jnp.argmax(pred,axis=1).reshape(-1,1)\n",
        "# print(pred_arg)"
      ],
      "metadata": {
        "id": "gcmnIU4h2Rwc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=1000,n_features=10, centers=3)\n",
        "X = jnp.asarray(X)\n",
        "y = jnp.asarray(y).reshape(-1,1)\n",
        "W = random.normal(key=random.PRNGKey(42),shape=(X.shape[-1],np.unique(y).shape[0]))\n",
        "b = random.normal(key=random.PRNGKey(42),shape=(np.unique(y).shape[0],1))\n",
        "\n",
        "X = (X - jnp.mean(X,axis=1).reshape(-1,1))/jnp.std(X,axis=1).reshape(-1,1)"
      ],
      "metadata": {
        "id": "vL1J4Ed-F991"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_sigmoid(matrix):\n",
        "    return (1/(1+jnp.exp(-matrix)))\n",
        "\n",
        "def custom_softmax(matrix):\n",
        "    return jnp.exp(matrix)/jnp.sum(jnp.exp(matrix),axis=1).reshape(-1,1)\n",
        "\n",
        "def forward_pass(X, W, b, activation):\n",
        "    mm = (jnp.matmul(X, W).T + b).T\n",
        "    # print(\"MM\",mm)\n",
        "    mm = (mm - jnp.mean(mm,axis=1).reshape(-1,1))/jnp.std(mm,axis=1).reshape(-1,1)\n",
        "    # print(\"MM AFTER\",mm)\n",
        "    # print(\"MM exp\", jnp.exp(mm))\n",
        "    # print(\"MAX \", jnp.max(mm,axis=1))\n",
        "    # print(\"BEFORE SOFTMAX \\n\",(jnp.matmul(X, W).T + b).T)\n",
        "    # print(\"FORWARD PASS \\n\",custom_softmax((jnp.matmul(X, W).T + b).T))\n",
        "    return activation(mm)\n",
        "\n",
        "def categorical_cross_entropy(yhat,y):\n",
        "    return -jnp.sum(y*jnp.log(yhat))\n",
        "\n",
        "def feed_forward(X, Y, W, b, activation):\n",
        "    Yhat = forward_pass(X,W,b)\n",
        "    Yhat = activation(Yhat)\n",
        "    categorical_pred = (jnp.argmax(Yhat,axis=1) + 1).reshape(-1,1)\n",
        "    return categorical_pred\n",
        "\n",
        "def loss(X,Y,W,b,loss_fnc, activation):\n",
        "    Yhat = forward_pass(X,W,b,activation)\n",
        "    prd = (jnp.argmax(fwd,axis=1)).reshape(-1,1)\n",
        "    return loss_fnc(Yhat,Y)\n",
        "\n",
        "for i in range(50):\n",
        "    print(\"W after update : \", np.isnan(W).sum())\n",
        "    print(\"b after update : \", np.isnan(W).sum())\n",
        "    fwd = forward_pass(X,W,b,custom_softmax)\n",
        "    print(fwd)\n",
        "    prd = (jnp.argmax(fwd,axis=1)).reshape(-1,1)\n",
        "    lss = categorical_cross_entropy(prd,y)\n",
        "    step_accuracy = (((prd==y).sum())/X.shape[0])*100\n",
        "    print(\"Step accuracy\",step_accuracy)\n",
        "    W_grad = grad(loss,argnums=2)(X,y,W,b,categorical_cross_entropy,custom_softmax)\n",
        "    # print('W_grad : \\n', W_grad)\n",
        "    print(\"W_grad nan value num : \", np.isnan(W_grad).sum())\n",
        "    b_grad = grad(loss,argnums=3)(X,y,W,b,categorical_cross_entropy,custom_softmax)\n",
        "    # print(\"b_grad : \\n\",b_grad)\n",
        "    print(\"b_grad nan value num : \", np.isnan(b_grad).sum())\n",
        "    print(\"W before update : \", np.isnan(W).sum())\n",
        "    print(\"b before update : \", np.isnan(W).sum())\n",
        "    W -= 0.1*W_grad\n",
        "    b -= 0.1*b_grad\n",
        "    # print(\"W : \\n\",W)\n",
        "    # print(\"b : \\n\",b)"
      ],
      "metadata": {
        "id": "qgWs6IWxIJMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57770cec-886e-4764-c32d-2cd97af5000e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.7802234  0.14826581 0.07151075]\n",
            " [0.795965   0.12558196 0.07845296]\n",
            " [0.0567133  0.41457665 0.52871007]\n",
            " ...\n",
            " [0.796628   0.12447157 0.07890042]\n",
            " [0.79617673 0.12522984 0.07859348]\n",
            " [0.7464574  0.18867871 0.06486399]]\n",
            "Step accuracy 0.0\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.13408683 0.7905008  0.07541236]\n",
            " [0.13633366 0.7889581  0.07470834]\n",
            " [0.07834465 0.7957997  0.1258557 ]\n",
            " ...\n",
            " [0.14047652 0.7860217  0.07350185]\n",
            " [0.1454772  0.782337   0.07218578]\n",
            " [0.11244056 0.8028204  0.0847391 ]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.14990954 0.7789596  0.07113082]\n",
            " [0.15340869 0.77622813 0.0703631 ]\n",
            " [0.08032357 0.7985334  0.12114312]\n",
            " ...\n",
            " [0.15812944 0.7724624  0.06940824]\n",
            " [0.16465311 0.76712435 0.06822252]\n",
            " [0.12013733 0.7990831  0.08077948]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.17280677 0.7602677  0.06692551]\n",
            " [0.17819631 0.75563973 0.0661639 ]\n",
            " [0.08312187 0.8015006  0.11537747]\n",
            " ...\n",
            " [0.18362479 0.750912   0.06546319]\n",
            " [0.19228822 0.74324745 0.0644644 ]\n",
            " [0.13109383 0.79249567 0.07641046]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.20493603 0.73183846 0.06322556]\n",
            " [0.21299879 0.72445357 0.06254761]\n",
            " [0.08702596 0.80427766 0.10869632]\n",
            " ...\n",
            " [0.21918721 0.7187357  0.06207707]\n",
            " [0.23061484 0.70807946 0.06130578]\n",
            " [0.14643566 0.781615   0.07194929]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.24688248 0.6927284  0.06038895]\n",
            " [0.258238   0.68191046 0.05985156]\n",
            " [0.09233724 0.8061978  0.10146487]\n",
            " ...\n",
            " [0.26506317 0.6753742  0.0595626 ]\n",
            " [0.2795169  0.66146004 0.05902311]\n",
            " [0.16707468 0.7651078  0.06781746]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.29543424 0.6460404  0.05852551]\n",
            " [0.3100193  0.6318378  0.05814303]\n",
            " [0.09927577 0.80648875 0.09423554]\n",
            " ...\n",
            " [0.31717312 0.62484944 0.05797751]\n",
            " [0.33414024 0.6082246  0.05763524]\n",
            " [0.19296828 0.74264014 0.0643916 ]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.34369645 0.5988335  0.05747011]\n",
            " [0.3605771  0.5822022  0.05722071]\n",
            " [0.10786536 0.80456275 0.08757187]\n",
            " ...\n",
            " [0.36773503 0.5751353  0.05712965]\n",
            " [0.38602477 0.5570429  0.05693229]\n",
            " [0.22256075 0.71560234 0.06183691]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.3851448  0.55791456 0.05694069]\n",
            " [0.40301472 0.5401954  0.05678988]\n",
            " [0.11789008 0.8002642  0.08184569]\n",
            " ...\n",
            " [0.41001588 0.53324246 0.05674164]\n",
            " [0.42839426 0.514964   0.05664181]\n",
            " [0.25325462 0.68666714 0.06007817]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.41704822 0.5262528  0.05669894]\n",
            " [0.4348378  0.50854665 0.05661554]\n",
            " [0.12895691 0.7938741  0.07716894]\n",
            " ...\n",
            " [0.44168913 0.5017184  0.05659242]\n",
            " [0.4593425  0.48410252 0.056555  ]\n",
            " [0.28260306 0.6584776  0.05891931]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.43995097 0.5034513  0.05659783]\n",
            " [0.4570205  0.48642135 0.05655811]\n",
            " [0.14060622 0.7859279  0.07346584]\n",
            " ...\n",
            " [0.4638041  0.47964537 0.05655048]\n",
            " [0.4803535  0.46309546 0.05655107]\n",
            " [0.3090607  0.63277316 0.05816626]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.45567888 0.4877609  0.05656018]\n",
            " [0.47172546 0.47172713 0.05654736]\n",
            " [0.15240794 0.77701485 0.07057719]\n",
            " ...\n",
            " [0.4785302  0.46492007 0.05654966]\n",
            " [0.49386638 0.44956172 0.05657183]\n",
            " [0.33200777 0.61031765 0.05767465]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4661135  0.47733754 0.05654893]\n",
            " [0.4810334  0.46241495 0.05655168]\n",
            " [0.16401894 0.7676495  0.06833158]\n",
            " ...\n",
            " [0.48792875 0.45551077 0.05656044]\n",
            " [0.5020737  0.44133273 0.0565935 ]\n",
            " [0.35145894 0.591192   0.05734908]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4727891  0.47066337 0.05654743]\n",
            " [0.4865816  0.45686    0.05655836]\n",
            " [0.17520061 0.7582208  0.06657864]\n",
            " ...\n",
            " [0.493614   0.4498147  0.05657126]\n",
            " [0.50664306 0.43674833 0.0566086 ]\n",
            " [0.36776057 0.5751102  0.05712935]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47685423 0.46659717 0.05654867]\n",
            " [0.48956776 0.45386904 0.05656323]\n",
            " [0.18580799 0.74899364 0.06519841]\n",
            " ...\n",
            " [0.49676394 0.44665727 0.0565787 ]\n",
            " [0.5087691  0.43461445 0.05661638]\n",
            " [0.38137916 0.56164294 0.05697785]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4791368  0.46431306 0.0565501 ]\n",
            " [0.49084175 0.45259258 0.05656558]\n",
            " [0.19576758 0.74013263 0.06409977]\n",
            " ...\n",
            " [0.4982141  0.44520348 0.05658244]\n",
            " [0.50928825 0.4340934  0.05661835]\n",
            " [0.39278194 0.55034685 0.0568712 ]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4802236  0.46322536 0.05655095]\n",
            " [0.49099767 0.45243645 0.05656588]\n",
            " [0.20505415 0.7317307  0.06321505]\n",
            " ...\n",
            " [0.49854815 0.4448684  0.05658333]\n",
            " [0.5087795  0.434604   0.05661641]\n",
            " [0.40238246 0.54082304 0.05679452]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.48052472 0.46292412 0.05655123]\n",
            " [0.49044648 0.45298865 0.05656481]\n",
            " [0.21367252 0.7238331  0.06249437]\n",
            " ...\n",
            " [0.49817058 0.44524702 0.05658234]\n",
            " [0.5076404  0.43574733 0.05661217]\n",
            " [0.41052416 0.53273755 0.05673835]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.48032388 0.463125   0.05655104]\n",
            " [0.4894695  0.45396736 0.05656305]\n",
            " [0.22164471 0.7164543  0.06190103]\n",
            " ...\n",
            " [0.49735844 0.44606125 0.0565802 ]\n",
            " [0.5061412  0.43725199 0.05660683]\n",
            " [0.41748276 0.5258208  0.05669649]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.479816   0.46363345 0.05655061]\n",
            " [0.4882573  0.45518166 0.05656097]\n",
            " [0.22900136 0.70959085 0.06140768]\n",
            " ...\n",
            " [0.49629995 0.44712248 0.05657754]\n",
            " [0.50446314 0.4389358  0.05660113]\n",
            " [0.42347565 0.5198595  0.05666482]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47913262 0.46431726 0.0565501 ]\n",
            " [0.4869367  0.45650437 0.05655888]\n",
            " [0.23577732 0.7032289  0.06099382]\n",
            " ...\n",
            " [0.49512064 0.44830462 0.0565747 ]\n",
            " [0.50272566 0.44067886 0.05659554]\n",
            " [0.42867315 0.5146863  0.05664056]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47836193 0.4650885  0.05654956]\n",
            " [0.48559055 0.4578525  0.05655694]\n",
            " [0.24200867 0.6973474  0.06064396]\n",
            " ...\n",
            " [0.4939031  0.44952494 0.05657192]\n",
            " [0.5010054  0.44240424 0.05659029]\n",
            " [0.4332085  0.5101697  0.05662175]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47756162 0.46588925 0.05654905]\n",
            " [0.48427126 0.45917356 0.05655521]\n",
            " [0.2477316  0.69192225 0.0603461 ]\n",
            " ...\n",
            " [0.4927001  0.4507306  0.05656932]\n",
            " [0.49935007 0.4440644  0.05658555]\n",
            " [0.43718693 0.5062059  0.05660704]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4767685  0.4666828  0.05654863]\n",
            " [0.48301083 0.46043536 0.05655371]\n",
            " [0.25298128 0.6869278  0.06009102]\n",
            " ...\n",
            " [0.491544   0.45188904 0.05656695]\n",
            " [0.49778712 0.4456315  0.05658132]\n",
            " [0.4406921  0.5027124  0.05659548]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4760055  0.46744612 0.05654828]\n",
            " [0.48182732 0.46162024 0.05655244]\n",
            " [0.2577915  0.6823372  0.05987129]\n",
            " ...\n",
            " [0.49045348 0.45298162 0.05656485]\n",
            " [0.49633077 0.4470916  0.05657761]\n",
            " [0.4437913  0.49962234 0.0565863 ]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4752857  0.46816626 0.05654799]\n",
            " [0.48072928 0.46271935 0.05655141]\n",
            " [0.26219502 0.67812395 0.05968115]\n",
            " ...\n",
            " [0.4894382  0.4539988  0.056563  ]\n",
            " [0.4949866  0.448439   0.05657437]\n",
            " [0.44653955 0.4968815  0.05657899]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4746158  0.46883643 0.05654779]\n",
            " [0.47971958 0.4637299  0.05655054]\n",
            " [0.26622275 0.6742615  0.05951584]\n",
            " ...\n",
            " [0.4885016  0.45493704 0.05656138]\n",
            " [0.49375436 0.44967398 0.05657157]\n",
            " [0.448982   0.49444488 0.05657312]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4739985  0.46945396 0.05654763]\n",
            " [0.4787969  0.46465325 0.05654985]\n",
            " [0.26990423 0.6707243  0.05937152]\n",
            " ...\n",
            " [0.48764333 0.45579666 0.05655997]\n",
            " [0.4926306  0.45080027 0.05656916]\n",
            " [0.45115685 0.4922748  0.05656843]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4734336  0.47001892 0.05654752]\n",
            " [0.47795776 0.46549287 0.05654929]\n",
            " [0.2732674  0.66748744 0.05924511]\n",
            " ...\n",
            " [0.4868606  0.45658055 0.05655878]\n",
            " [0.49160922 0.45182362 0.05656707]\n",
            " [0.45309597 0.49033937 0.05656464]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47291955 0.470533   0.05654744]\n",
            " [0.4771974  0.46625373 0.05654885]\n",
            " [0.27633846 0.6645275  0.05913398]\n",
            " ...\n",
            " [0.48614952 0.45729277 0.05655774]\n",
            " [0.49068376 0.4527509  0.05656527]\n",
            " [0.4548271  0.4886113  0.05656157]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4724536  0.470999   0.0565474 ]\n",
            " [0.4765102  0.46694118 0.05654851]\n",
            " [0.27914217 0.6618219  0.05903599]\n",
            " ...\n",
            " [0.48550507 0.45793816 0.05655682]\n",
            " [0.48984686 0.45358947 0.05656373]\n",
            " [0.45637375 0.48706713 0.05655909]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47203252 0.47142014 0.05654738]\n",
            " [0.47589052 0.46756113 0.05654823]\n",
            " [0.28170136 0.65934926 0.05894927]\n",
            " ...\n",
            " [0.48492223 0.45852178 0.05655603]\n",
            " [0.48909095 0.4543467  0.05656239]\n",
            " [0.45775664 0.48568627 0.05655705]]\n",
            "Step accuracy 33.3\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47165284 0.47179982 0.05654737]\n",
            " [0.47533253 0.46811938 0.05654802]\n",
            " [0.2840376  0.65708995 0.05887232]\n",
            " ...\n",
            " [0.48439577 0.4590489  0.05655535]\n",
            " [0.48840907 0.45502973 0.05656123]\n",
            " [0.45899373 0.48445088 0.05655542]]\n",
            "Step accuracy 32.8\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47131115 0.4721414  0.05654738]\n",
            " [0.47483072 0.46862137 0.05654786]\n",
            " [0.2861709  0.65502524 0.0588039 ]\n",
            " ...\n",
            " [0.4839207  0.45952445 0.05655475]\n",
            " [0.48779443 0.45564538 0.05656022]\n",
            " [0.46010092 0.48334494 0.05655407]]\n",
            "Step accuracy 32.7\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47100425 0.47244838 0.05654741]\n",
            " [0.47437987 0.46907246 0.05654773]\n",
            " [0.28811944 0.6531377  0.05874282]\n",
            " ...\n",
            " [0.48349243 0.45995328 0.05655424]\n",
            " [0.48724055 0.45620012 0.05655935]\n",
            " [0.46109214 0.48235482 0.05655298]]\n",
            "Step accuracy 32.5\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47072867 0.4727239  0.05654743]\n",
            " [0.47397503 0.46947736 0.05654762]\n",
            " [0.28990036 0.65141135 0.05868822]\n",
            " ...\n",
            " [0.4831063  0.4603398  0.0565538 ]\n",
            " [0.48674163 0.45669976 0.05655858]\n",
            " [0.46197987 0.48146808 0.05655207]]\n",
            "Step accuracy 31.7\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47048166 0.47297096 0.05654744]\n",
            " [0.47361177 0.4698406  0.05654756]\n",
            " [0.29152918 0.6498316  0.05863925]\n",
            " ...\n",
            " [0.48275843 0.4606881  0.05655341]\n",
            " [0.4862922  0.45714983 0.05655792]\n",
            " [0.46277505 0.4806736  0.05655134]]\n",
            "Step accuracy 31.100002\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47026032 0.47319224 0.05654747]\n",
            " [0.4732861  0.47016644 0.0565475 ]\n",
            " [0.29302025 0.6483846  0.05859522]\n",
            " ...\n",
            " [0.48244497 0.461002   0.05655308]\n",
            " [0.4858874  0.4575552  0.05655734]\n",
            " [0.46348745 0.47996184 0.05655073]]\n",
            "Step accuracy 30.400002\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.47006208 0.47339034 0.05654752]\n",
            " [0.472994   0.4704585  0.05654746]\n",
            " [0.29438657 0.64705795 0.05855551]\n",
            " ...\n",
            " [0.48216233 0.46128485 0.05655279]\n",
            " [0.48552272 0.45792034 0.05655684]\n",
            " [0.4641258  0.47932398 0.05655023]]\n",
            "Step accuracy 29.800001\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4698849  0.47356766 0.05654755]\n",
            " [0.47273228 0.4707203  0.05654743]\n",
            " [0.29564008 0.6458402  0.05851965]\n",
            " ...\n",
            " [0.4819076  0.46153983 0.05655254]\n",
            " [0.48519412 0.45824942 0.05655639]\n",
            " [0.46469784 0.4787523  0.05654982]]\n",
            "Step accuracy 29.1\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.46972632 0.47372612 0.05654757]\n",
            " [0.47249785 0.47095472 0.0565474 ]\n",
            " [0.29679182 0.644721   0.05848715]\n",
            " ...\n",
            " [0.4816779  0.46176973 0.05655229]\n",
            " [0.48489788 0.45854613 0.056556  ]\n",
            " [0.46521062 0.47823986 0.05654947]]\n",
            "Step accuracy 28.400002\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.46958458 0.4738677  0.05654759]\n",
            " [0.4722878  0.4711648  0.05654739]\n",
            " [0.2978516  0.6436908  0.05845762]\n",
            " ...\n",
            " [0.48147064 0.46197724 0.0565521 ]\n",
            " [0.48463064 0.4588137  0.05655565]\n",
            " [0.46567026 0.47778055 0.05654919]]\n",
            "Step accuracy 27.100002\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.46945822 0.47399423 0.05654764]\n",
            " [0.47209966 0.47135288 0.05654738]\n",
            " [0.29882833 0.64274096 0.05843072]\n",
            " ...\n",
            " [0.48128358 0.4621645  0.05655191]\n",
            " [0.4843895  0.45905513 0.05655533]\n",
            " [0.46608236 0.4773687  0.05654895]]\n",
            "Step accuracy 26.500002\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.46934524 0.4741071  0.05654765]\n",
            " [0.47193125 0.47152135 0.05654738]\n",
            " [0.2997302  0.6418637  0.05840615]\n",
            " ...\n",
            " [0.48111466 0.4623336  0.05655176]\n",
            " [0.4841718  0.45927316 0.05655506]\n",
            " [0.46645185 0.47699946 0.05654874]]\n",
            "Step accuracy 25.6\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.46924454 0.4742078  0.05654768]\n",
            " [0.47178048 0.47167212 0.05654738]\n",
            " [0.30056465 0.64105165 0.05838362]\n",
            " ...\n",
            " [0.48096183 0.46248657 0.05655161]\n",
            " [0.483975   0.45947015 0.05655483]\n",
            " [0.46678314 0.4766682  0.05654857]]\n",
            "Step accuracy 24.7\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4691547  0.47429755 0.05654771]\n",
            " [0.47164562 0.47180703 0.05654737]\n",
            " [0.30133843 0.6402986  0.05836293]\n",
            " ...\n",
            " [0.48082373 0.46262485 0.05655149]\n",
            " [0.4837971  0.45964834 0.05655462]\n",
            " [0.46708024 0.47637126 0.05654844]]\n",
            "Step accuracy 24.300001\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4690748  0.4743775  0.05654772]\n",
            " [0.47152483 0.47192776 0.05654738]\n",
            " [0.3020574  0.63959867 0.05834388]\n",
            " ...\n",
            " [0.4806985  0.46275005 0.05655138]\n",
            " [0.483636   0.4598096  0.05655441]\n",
            " [0.46734685 0.47610492 0.05654832]]\n",
            "Step accuracy 23.700003\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.46900353 0.47444868 0.05654773]\n",
            " [0.4714168  0.4720358  0.05654738]\n",
            " [0.3027272  0.6389465  0.05832627]\n",
            " ...\n",
            " [0.48058507 0.46286356 0.05655127]\n",
            " [0.48349008 0.45995578 0.05655425]\n",
            " [0.4675859  0.47586593 0.05654821]]\n",
            "Step accuracy 23.0\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.4689403  0.47451195 0.05654776]\n",
            " [0.47132015 0.47213238 0.05654738]\n",
            " [0.30335265 0.6383374  0.05830995]\n",
            " ...\n",
            " [0.48048216 0.4629666  0.05655118]\n",
            " [0.48335764 0.46008828 0.05655409]\n",
            " [0.46780038 0.4756515  0.05654812]]\n",
            "Step accuracy 22.200003\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n",
            "W after update :  0\n",
            "b after update :  0\n",
            "[[0.46888393 0.47456822 0.05654777]\n",
            " [0.4712338  0.47221884 0.05654738]\n",
            " [0.30393827 0.63776696 0.05829478]\n",
            " ...\n",
            " [0.48038864 0.46306023 0.05655111]\n",
            " [0.4832374  0.46020868 0.05655395]\n",
            " [0.4679928  0.47545904 0.05654806]]\n",
            "Step accuracy 21.400002\n",
            "W_grad nan value num :  0\n",
            "b_grad nan value num :  0\n",
            "W before update :  0\n",
            "b before update :  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(W_grad).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLQgJiTdIzpW",
        "outputId": "de7b6d8d-d643-4481-cdf9-dda51552a261"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, class_num=1, activation_func='sigmoid', loss_func='mean_squared_error', learning_rate=0.01, iter=50, random_key=1):\n",
        "        self.class_num = class_num\n",
        "        self.activation_func = activation_func\n",
        "        self.loss_func = loss_func\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iter = iter\n",
        "        self.random_key = random.PRNGKey(random_key)\n",
        "\n",
        "    def initialize_weights(self, X):\n",
        "        self.W = random.normal(\n",
        "            key=self.random_key,\n",
        "            shape=(X.shape[-1],self.class_num)\n",
        "        )\n",
        "        self.b = random.normal(\n",
        "            key=self.random_key,\n",
        "            shape=(self.class_num,1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return (jnp.matmul(X, self.W).T + self.b).T\n",
        "\n",
        "    def softmax_func(self, matrix):\n",
        "        return jnp.exp(matrix)/jnp.sum(jnp.exp(matrix),axis=1).reshape(-1,1)\n",
        "\n",
        "    def sigmoid_func(self, matrix):\n",
        "        return (1/(1+jnp.exp(-matrix)))\n",
        "\n",
        "    def jax_sigmoid(self, matrix):\n",
        "        return jax.nn.sigmoid(matrix)\n",
        "    \n",
        "    def mean_squared_error(self, Yhat, Y):\n",
        "        return jnp.mean((Yhat-Y)**2)\n",
        "        \n",
        "    def categorical_cross_entropy(self, Yhat, Y):\n",
        "        return -jnp.sum(Y*jnp.log(Yhat))\n",
        "    \n",
        "    def forward_step(self, X, W, b):\n",
        "        if self.activation_func == 'sigmoid':\n",
        "            return self.sigmoid_func((jnp.matmul(X, W).T + b).T)\n",
        "        elif self.activation_func == 'softmax':\n",
        "            # print(\"IN FORWARD \\n\",((jnp.matmul(X, W).T + b).T))\n",
        "            print(\"WITH THE SOFT \\n\",jax.nn.softmax((jnp.matmul(X, W).T + b).T))\n",
        "            # return self.softmax_func((jnp.matmul(X, W).T + b).T)\n",
        "            return jax.nn.softmax((jnp.matmul(X, W).T + b).T)\n",
        "        else:\n",
        "            return (jnp.matmul(X, W).T + b).T\n",
        "    \n",
        "    def loss(self, X, Y, W, b):\n",
        "        # print(X.shape,Y.shape,W.shape,b.shape)\n",
        "        Yhat = self.forward_step(X,W,b)\n",
        "        # print(\"In loss function \\n\",Yhat)\n",
        "        if self.loss_func == 'mean_squared_error':\n",
        "            return self.mean_squared_error(Yhat, Y)\n",
        "        elif self.loss_func == 'categorical_cross_entropy':\n",
        "            return self.categorical_cross_entropy(Yhat,Y)\n",
        "\n",
        "    def param_update(self, w_gradient, b_gradient):\n",
        "        self.W -= self.learning_rate * w_gradient\n",
        "        self.b -= self.learning_rate * b_gradient\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return jnp.argmax(self.forward_step(X,self.W,self.b),axis=1)\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        self.history = {\n",
        "            'loss':[],\n",
        "            'accuracy':[]\n",
        "        }\n",
        "        self.initialize_weights(X)\n",
        "        for i in range(self.iter):\n",
        "            # print(X,Y,self.W,self.b)\n",
        "            step_loss = self.loss(X, Y, self.W, self.b)\n",
        "            # print(step_loss)\n",
        "            step_prediction = self.predict(X).reshape(-1,1)\n",
        "            step_accuracy = (((step_prediction==Y).sum())/X.shape[0])*100\n",
        "            self.history['accuracy'].append(step_accuracy)\n",
        "            self.history['loss'].append(step_loss)\n",
        "            W_grad = grad(self.loss,argnums=2)(X,Y,self.W,self.b)\n",
        "            b_grad = grad(self.loss,argnums=3)(X,Y,self.W,self.b)\n",
        "            self.param_update(W_grad,b_grad)\n",
        "            print(\"Epoch\",i,\"accuracy :\",step_accuracy)\n",
        "            # print(step_loss)\n",
        "            # print(W_grad)\n",
        "            # print(b_grad)\n",
        "            # print(self.W)\n",
        "        return self.history"
      ],
      "metadata": {
        "id": "y8qZiC_MwKYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron_model = Perceptron(\n",
        "    class_num=class_num,\n",
        "    activation_func='softmax',\n",
        "    loss_func='categorical_cross_entropy',\n",
        ")"
      ],
      "metadata": {
        "id": "B04pMvXzjJXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = perceptron_model.fit(X,y)\n",
        "plt.plot(model_history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "hf7Kl0r4kbGL",
        "outputId": "783e84a8-775d-4285-868a-cb828db438e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f273b1d70d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(train_X, train_Y), (test_X, test_Y) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtUwhIqsm7Kz",
        "outputId": "4735e16e-8310-4a71-e812-f78af674a273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train X shapes : \", train_X.shape, type(train_X))\n",
        "print(\"Train Y shapes : \", train_Y.shape, type(train_Y))\n",
        "print(\"Test X shapes : \", test_X.shape, type(test_X))\n",
        "print(\"Test Y shapes : \", test_Y.shape, type(test_Y))\n",
        "\n",
        "flat_train_X = jnp.asarray(train_X).reshape(-1,28*28)\n",
        "flat_norm_train_X = flat_train_X/255.0\n",
        "flat_train_Y = jnp.asarray(train_Y).reshape(-1,1)\n",
        "\n",
        "\n",
        "flat_test_X = jnp.asarray(test_X).reshape(-1,28*28)\n",
        "flat_norm_test_X = flat_test_X/255.0\n",
        "flat_test_Y = jnp.asarray(test_Y).reshape(-1,1)\n",
        "\n",
        "print(\"Processed Train X shapes : \", flat_norm_train_X.shape, type(flat_norm_train_X))\n",
        "print(\"Processed Train Y shapes : \", flat_train_Y.shape, type(flat_train_Y))\n",
        "print(\"Processed Test X shapes : \", flat_norm_test_X.shape, type(flat_norm_test_X))\n",
        "print(\"Processed Test Y shapes : \", flat_test_Y.shape, type(flat_test_Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHXh70ZZnT4r",
        "outputId": "3e59b405-63b0-4335-d8ba-257200b54e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X shapes :  (60000, 28, 28) <class 'numpy.ndarray'>\n",
            "Train Y shapes :  (60000,) <class 'numpy.ndarray'>\n",
            "Test X shapes :  (10000, 28, 28) <class 'numpy.ndarray'>\n",
            "Test Y shapes :  (10000,) <class 'numpy.ndarray'>\n",
            "Processed Train X shapes :  (60000, 784) <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "Processed Train Y shapes :  (60000, 1) <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "Processed Test X shapes :  (10000, 784) <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "Processed Test Y shapes :  (10000, 1) <class 'jaxlib.xla_extension.DeviceArray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_perceptron_model = Perceptron(\n",
        "    class_num=10,\n",
        "    activation_func='softmax',\n",
        "    loss_func='categorical_cross_entropy',\n",
        "    iter=3\n",
        ")"
      ],
      "metadata": {
        "id": "4eIDePdoteRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_history = mnist_perceptron_model.fit(flat_norm_train_X,flat_train_Y)\n",
        "plt.plot(model_history['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QaUGaWvVnnQy",
        "outputId": "e4e82a6b-5dfc-4317-a4fe-e99da654b092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WITH THE SOFT \n",
            " [[6.41975999e-02 7.72180375e-14 4.40844923e-01 ... 7.25509149e-12\n",
            "  3.33175086e-03 2.19234789e-06]\n",
            " [1.12821253e-05 4.41953924e-15 2.73078695e-12 ... 1.67155872e-12\n",
            "  5.13250828e-02 4.23662883e-09]\n",
            " [5.91588056e-10 1.31975125e-11 4.27631810e-16 ... 7.81825861e-07\n",
            "  2.30332850e-10 2.73715359e-05]\n",
            " ...\n",
            " [1.20018827e-04 6.17792661e-09 3.30750964e-07 ... 4.33711648e-05\n",
            "  1.53182792e-02 2.73711365e-08]\n",
            " [3.71682213e-10 4.85025253e-16 5.07619077e-12 ... 6.21370873e-07\n",
            "  2.71865545e-04 4.64026018e-09]\n",
            " [2.05249432e-13 1.14529432e-15 2.50863080e-10 ... 6.78584272e-11\n",
            "  1.15804432e-04 2.59300936e-08]]\n",
            "WITH THE SOFT \n",
            " [[6.41975999e-02 7.72180375e-14 4.40844923e-01 ... 7.25509149e-12\n",
            "  3.33175086e-03 2.19234789e-06]\n",
            " [1.12821253e-05 4.41953924e-15 2.73078695e-12 ... 1.67155872e-12\n",
            "  5.13250828e-02 4.23662883e-09]\n",
            " [5.91588056e-10 1.31975125e-11 4.27631810e-16 ... 7.81825861e-07\n",
            "  2.30332850e-10 2.73715359e-05]\n",
            " ...\n",
            " [1.20018827e-04 6.17792661e-09 3.30750964e-07 ... 4.33711648e-05\n",
            "  1.53182792e-02 2.73711365e-08]\n",
            " [3.71682213e-10 4.85025253e-16 5.07619077e-12 ... 6.21370873e-07\n",
            "  2.71865545e-04 4.64026018e-09]\n",
            " [2.05249432e-13 1.14529432e-15 2.50863080e-10 ... 6.78584272e-11\n",
            "  1.15804432e-04 2.59300936e-08]]\n",
            "WITH THE SOFT \n",
            " Traced<ConcreteArray([[6.41975999e-02 7.72180375e-14 4.40844923e-01 ... 7.25509149e-12\n",
            "  3.33175086e-03 2.19234789e-06]\n",
            " [1.12821253e-05 4.41953924e-15 2.73078695e-12 ... 1.67155872e-12\n",
            "  5.13250828e-02 4.23662883e-09]\n",
            " [5.91588056e-10 1.31975125e-11 4.27631810e-16 ... 7.81825861e-07\n",
            "  2.30332850e-10 2.73715359e-05]\n",
            " ...\n",
            " [1.20018827e-04 6.17792661e-09 3.30750964e-07 ... 4.33711648e-05\n",
            "  1.53182792e-02 2.73711365e-08]\n",
            " [3.71682213e-10 4.85025253e-16 5.07619077e-12 ... 6.21370873e-07\n",
            "  2.71865545e-04 4.64026018e-09]\n",
            " [2.05249432e-13 1.14529432e-15 2.50863080e-10 ... 6.78584272e-11\n",
            "  1.15804432e-04 2.59300936e-08]], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = DeviceArray([[6.41975999e-02, 7.72180375e-14, 4.40844923e-01, ...,\n",
            "              7.25509149e-12, 3.33175086e-03, 2.19234789e-06],\n",
            "             [1.12821253e-05, 4.41953924e-15, 2.73078695e-12, ...,\n",
            "              1.67155872e-12, 5.13250828e-02, 4.23662883e-09],\n",
            "             [5.91588056e-10, 1.31975125e-11, 4.27631810e-16, ...,\n",
            "              7.81825861e-07, 2.30332850e-10, 2.73715359e-05],\n",
            "             ...,\n",
            "             [1.20018827e-04, 6.17792661e-09, 3.30750964e-07, ...,\n",
            "              4.33711648e-05, 1.53182792e-02, 2.73711365e-08],\n",
            "             [3.71682213e-10, 4.85025253e-16, 5.07619077e-12, ...,\n",
            "              6.21370873e-07, 2.71865545e-04, 4.64026018e-09],\n",
            "             [2.05249432e-13, 1.14529432e-15, 2.50863080e-10, ...,\n",
            "              6.78584272e-11, 1.15804432e-04, 2.59300936e-08]],            dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[60000,10])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[60000,10]), *)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f273b6f0610>, invars=(Traced<ConcreteArray([[2.0423765]\n",
            " [1.0541203]\n",
            " [1.0000284]\n",
            " ...\n",
            " [1.0157268]\n",
            " [1.0002725]\n",
            " [1.0001187]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[1.31115675e-01 1.57708306e-13 9.00371313e-01 ... 1.48176280e-11\n",
            "  6.80468976e-03 4.47759976e-06]\n",
            " [1.18927173e-05 4.65872610e-15 2.87857780e-12 ... 1.76202390e-12\n",
            "  5.41028120e-02 4.46591653e-09]\n",
            " [5.91604821e-10 1.31978872e-11 4.27643933e-16 ... 7.81848030e-07\n",
            "  2.30339386e-10 2.73723126e-05]\n",
            " ...\n",
            " [1.21906342e-04 6.27508578e-09 3.35952620e-07 ... 4.40532567e-05\n",
            "  1.55591872e-02 2.78015975e-08]\n",
            " [3.71783493e-10 4.85157443e-16 5.07757421e-12 ... 6.21540210e-07\n",
            "  2.71939643e-04 4.64152494e-09]\n",
            " [2.05273800e-13 1.14543027e-15 2.50892862e-10 ... 6.78664833e-11\n",
            "  1.15818184e-04 2.59331721e-08]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[0.23973331]\n",
            " [0.89995265]\n",
            " [0.99994326]\n",
            " ...\n",
            " [0.9692731 ]\n",
            " [0.99945515]\n",
            " [0.9997626 ]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,10]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,1]):JaxprTrace(level=1/0)>), outvars=[<weakref at 0x7f27382c84d0; to 'JaxprTracer' at 0x7f27382c8410>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(true_divide)', 'donated_invars': (False, False, False, False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[60000,1] b:f32[60000,10] c:f32[60000,1] d:f32[60000,10] e:f32[60000,1]. let\n",
            "    f:f32[60000,10] = div d a\n",
            "    g:f32[60000,1] = neg e\n",
            "    h:f32[60000,10] = mul g b\n",
            "    i:f32[60000,10] = mul h c\n",
            "    j:f32[60000,10] = add_any f i\n",
            "  in (j,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7f27382ea070>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
            "WITH THE SOFT \n",
            " Traced<ConcreteArray([[6.41975999e-02 7.72180375e-14 4.40844923e-01 ... 7.25509149e-12\n",
            "  3.33175086e-03 2.19234789e-06]\n",
            " [1.12821253e-05 4.41953924e-15 2.73078695e-12 ... 1.67155872e-12\n",
            "  5.13250828e-02 4.23662883e-09]\n",
            " [5.91588056e-10 1.31975125e-11 4.27631810e-16 ... 7.81825861e-07\n",
            "  2.30332850e-10 2.73715359e-05]\n",
            " ...\n",
            " [1.20018827e-04 6.17792661e-09 3.30750964e-07 ... 4.33711648e-05\n",
            "  1.53182792e-02 2.73711365e-08]\n",
            " [3.71682213e-10 4.85025253e-16 5.07619077e-12 ... 6.21370873e-07\n",
            "  2.71865545e-04 4.64026018e-09]\n",
            " [2.05249432e-13 1.14529432e-15 2.50863080e-10 ... 6.78584272e-11\n",
            "  1.15804432e-04 2.59300936e-08]], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = DeviceArray([[6.41975999e-02, 7.72180375e-14, 4.40844923e-01, ...,\n",
            "              7.25509149e-12, 3.33175086e-03, 2.19234789e-06],\n",
            "             [1.12821253e-05, 4.41953924e-15, 2.73078695e-12, ...,\n",
            "              1.67155872e-12, 5.13250828e-02, 4.23662883e-09],\n",
            "             [5.91588056e-10, 1.31975125e-11, 4.27631810e-16, ...,\n",
            "              7.81825861e-07, 2.30332850e-10, 2.73715359e-05],\n",
            "             ...,\n",
            "             [1.20018827e-04, 6.17792661e-09, 3.30750964e-07, ...,\n",
            "              4.33711648e-05, 1.53182792e-02, 2.73711365e-08],\n",
            "             [3.71682213e-10, 4.85025253e-16, 5.07619077e-12, ...,\n",
            "              6.21370873e-07, 2.71865545e-04, 4.64026018e-09],\n",
            "             [2.05249432e-13, 1.14529432e-15, 2.50863080e-10, ...,\n",
            "              6.78584272e-11, 1.15804432e-04, 2.59300936e-08]],            dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[60000,10])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[60000,10]), *)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f273b6f0240>, invars=(Traced<ConcreteArray([[2.0423765]\n",
            " [1.0541203]\n",
            " [1.0000284]\n",
            " ...\n",
            " [1.0157268]\n",
            " [1.0002725]\n",
            " [1.0001187]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[1.31115675e-01 1.57708306e-13 9.00371313e-01 ... 1.48176280e-11\n",
            "  6.80468976e-03 4.47759976e-06]\n",
            " [1.18927173e-05 4.65872610e-15 2.87857780e-12 ... 1.76202390e-12\n",
            "  5.41028120e-02 4.46591653e-09]\n",
            " [5.91604821e-10 1.31978872e-11 4.27643933e-16 ... 7.81848030e-07\n",
            "  2.30339386e-10 2.73723126e-05]\n",
            " ...\n",
            " [1.21906342e-04 6.27508578e-09 3.35952620e-07 ... 4.40532567e-05\n",
            "  1.55591872e-02 2.78015975e-08]\n",
            " [3.71783493e-10 4.85157443e-16 5.07757421e-12 ... 6.21540210e-07\n",
            "  2.71939643e-04 4.64152494e-09]\n",
            " [2.05273800e-13 1.14543027e-15 2.50892862e-10 ... 6.78664833e-11\n",
            "  1.15818184e-04 2.59331721e-08]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[0.23973331]\n",
            " [0.89995265]\n",
            " [0.99994326]\n",
            " ...\n",
            " [0.9692731 ]\n",
            " [0.99945515]\n",
            " [0.9997626 ]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,10]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,1]):JaxprTrace(level=1/0)>), outvars=[<weakref at 0x7f27382c8290; to 'JaxprTracer' at 0x7f27382c8410>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(true_divide)', 'donated_invars': (False, False, False, False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[60000,1] b:f32[60000,10] c:f32[60000,1] d:f32[60000,10] e:f32[60000,1]. let\n",
            "    f:f32[60000,10] = div d a\n",
            "    g:f32[60000,1] = neg e\n",
            "    h:f32[60000,10] = mul g b\n",
            "    i:f32[60000,10] = mul h c\n",
            "    j:f32[60000,10] = add_any f i\n",
            "  in (j,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7f27382f05b0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
            "Epoch 0 accuracy : 8.281667\n",
            "WITH THE SOFT \n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            "WITH THE SOFT \n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            "WITH THE SOFT \n",
            " Traced<ConcreteArray([[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = DeviceArray([[0., 1., 0., ..., 0., 0., 0.],\n",
            "             [0., 1., 0., ..., 0., 0., 0.],\n",
            "             [0., 1., 0., ..., 0., 0., 0.],\n",
            "             ...,\n",
            "             [0., 1., 0., ..., 0., 0., 0.],\n",
            "             [0., 1., 0., ..., 0., 0., 0.],\n",
            "             [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[60000,10])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[60000,10]), *)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f273b6f04f0>, invars=(Traced<ConcreteArray([[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,10]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,1]):JaxprTrace(level=1/0)>), outvars=[<weakref at 0x7f27382c8770; to 'JaxprTracer' at 0x7f27382c86b0>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(true_divide)', 'donated_invars': (False, False, False, False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[60000,1] b:f32[60000,10] c:f32[60000,1] d:f32[60000,10] e:f32[60000,1]. let\n",
            "    f:f32[60000,10] = div d a\n",
            "    g:f32[60000,1] = neg e\n",
            "    h:f32[60000,10] = mul g b\n",
            "    i:f32[60000,10] = mul h c\n",
            "    j:f32[60000,10] = add_any f i\n",
            "  in (j,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7f27382f0330>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
            "WITH THE SOFT \n",
            " Traced<ConcreteArray([[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = DeviceArray([[0., 1., 0., ..., 0., 0., 0.],\n",
            "             [0., 1., 0., ..., 0., 0., 0.],\n",
            "             [0., 1., 0., ..., 0., 0., 0.],\n",
            "             ...,\n",
            "             [0., 1., 0., ..., 0., 0., 0.],\n",
            "             [0., 1., 0., ..., 0., 0., 0.],\n",
            "             [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[60000,10])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[60000,10]), *)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f273b6f0610>, invars=(Traced<ConcreteArray([[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,10]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,1]):JaxprTrace(level=1/0)>), outvars=[<weakref at 0x7f27382c8470; to 'JaxprTracer' at 0x7f27382c8650>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(true_divide)', 'donated_invars': (False, False, False, False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[60000,1] b:f32[60000,10] c:f32[60000,1] d:f32[60000,10] e:f32[60000,1]. let\n",
            "    f:f32[60000,10] = div d a\n",
            "    g:f32[60000,1] = neg e\n",
            "    h:f32[60000,10] = mul g b\n",
            "    i:f32[60000,10] = mul h c\n",
            "    j:f32[60000,10] = add_any f i\n",
            "  in (j,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7f27382dd9b0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
            "Epoch 1 accuracy : 11.236667\n",
            "WITH THE SOFT \n",
            " [[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "WITH THE SOFT \n",
            " [[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "WITH THE SOFT \n",
            " Traced<ConcreteArray([[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = DeviceArray([[nan, nan, nan, ..., nan, nan, nan],\n",
            "             [nan, nan, nan, ..., nan, nan, nan],\n",
            "             [nan, nan, nan, ..., nan, nan, nan],\n",
            "             ...,\n",
            "             [nan, nan, nan, ..., nan, nan, nan],\n",
            "             [nan, nan, nan, ..., nan, nan, nan],\n",
            "             [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[60000,10])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[60000,10]), *)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f273b6f0620>, invars=(Traced<ConcreteArray([[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " ...\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " ...\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,10]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,1]):JaxprTrace(level=1/0)>), outvars=[<weakref at 0x7f27382c8830; to 'JaxprTracer' at 0x7f27382c8350>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(true_divide)', 'donated_invars': (False, False, False, False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[60000,1] b:f32[60000,10] c:f32[60000,1] d:f32[60000,10] e:f32[60000,1]. let\n",
            "    f:f32[60000,10] = div d a\n",
            "    g:f32[60000,1] = neg e\n",
            "    h:f32[60000,10] = mul g b\n",
            "    i:f32[60000,10] = mul h c\n",
            "    j:f32[60000,10] = add_any f i\n",
            "  in (j,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7f27382f0ef0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
            "WITH THE SOFT \n",
            " Traced<ConcreteArray([[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = DeviceArray([[nan, nan, nan, ..., nan, nan, nan],\n",
            "             [nan, nan, nan, ..., nan, nan, nan],\n",
            "             [nan, nan, nan, ..., nan, nan, nan],\n",
            "             ...,\n",
            "             [nan, nan, nan, ..., nan, nan, nan],\n",
            "             [nan, nan, nan, ..., nan, nan, nan],\n",
            "             [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[60000,10])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[60000,10]), *)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f273b6f0340>, invars=(Traced<ConcreteArray([[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " ...\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray([[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " ...\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]], dtype=float32):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,10]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[60000,1]):JaxprTrace(level=1/0)>), outvars=[<weakref at 0x7f27382c81d0; to 'JaxprTracer' at 0x7f27382c89b0>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(true_divide)', 'donated_invars': (False, False, False, False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[60000,1] b:f32[60000,10] c:f32[60000,1] d:f32[60000,10] e:f32[60000,1]. let\n",
            "    f:f32[60000,10] = div d a\n",
            "    g:f32[60000,1] = neg e\n",
            "    h:f32[60000,10] = mul g b\n",
            "    i:f32[60000,10] = mul h c\n",
            "    j:f32[60000,10] = add_any f i\n",
            "  in (j,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7f27383d5df0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
            "Epoch 2 accuracy : 9.871667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f273825ac90>]"
            ]
          },
          "metadata": {},
          "execution_count": 156
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_perceptron_model.W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Uyv8uOw7GL",
        "outputId": "2e0315b9-18f9-452f-96fd-751110637598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[nan, nan, nan, ..., nan, nan, nan],\n",
              "             [nan, nan, nan, ..., nan, nan, nan],\n",
              "             [nan, nan, nan, ..., nan, nan, nan],\n",
              "             ...,\n",
              "             [nan, nan, nan, ..., nan, nan, nan],\n",
              "             [nan, nan, nan, ..., nan, nan, nan],\n",
              "             [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    }
  ]
}